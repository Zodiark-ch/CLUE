#!/usr/bin/env python
# coding=utf-8
"""
WMDPæ•°æ®é›†åŠ è½½å’Œå¤„ç†è„šæœ¬
ç”¨äºé‡æ–°å¤„ç†WMDPæ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯cyberå­é›†
"""

import os
import sys
import torch
from datasets import load_dataset, load_from_disk
from typing import Optional, Dict, Any
os.environ["CUDA_VISIBLE_DEVICES"] = "3"
def load_wmdp_dataset(
    dataset_name: str = "allenai/winogrande",
    subset: str = "winogrande_debiased",
    split: str = "train",
    cache_dir: str = "./.cache"
) -> Any:
    """
    åŠ è½½WMDPæ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°ï¼Œé»˜è®¤ä¸º "cais/wmdp"
        subset: æ•°æ®é›†å­é›†ï¼Œé»˜è®¤ä¸º "wmdp-cyber"
        split: æ•°æ®é›†åˆ†å‰²ï¼Œé»˜è®¤ä¸º "test"
        cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º "./.cache"
    
    Returns:
        åŠ è½½çš„æ•°æ®é›†
    """
    try:
        print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name}, åˆ†å‰²: {split}")
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset(
            dataset_name, 
            subset,
            cache_dir=cache_dir
        )[split]
        
        print(f"æˆåŠŸåŠ è½½æ•°æ®é›†ï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        print(f"æ•°æ®é›†ç‰¹å¾: {dataset.features}")
        
        return dataset
        
    except Exception as e:
        print(f"åŠ è½½æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def save_dataset_to_disk(
    dataset: Any,
    save_path: str,
    dataset_name: str = "winogrande"
) -> None:
    """
    å°†æ•°æ®é›†ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨load_from_diskåŠ è½½
    
    Args:
        dataset: è¦ä¿å­˜çš„æ•°æ®é›†
        save_path: ä¿å­˜è·¯å¾„
        dataset_name: æ•°æ®é›†åç§°ï¼Œç”¨äºåˆ›å»ºç›®å½•
    """
    try:
        # åˆ›å»ºä¿å­˜ç›®å½•
        full_save_path = os.path.join(save_path, dataset_name)
        os.makedirs(full_save_path, exist_ok=True)
        
        print(f"æ­£åœ¨ä¿å­˜æ•°æ®é›†åˆ°: {full_save_path}")
        
        # ä¿å­˜æ•°æ®é›†
        dataset.save_to_disk(full_save_path)
        
        print(f"æ•°æ®é›†å·²æˆåŠŸä¿å­˜åˆ°: {full_save_path}")
        print(f"ä¿å­˜çš„æ•°æ®é›†åŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        
    except Exception as e:
        print(f"ä¿å­˜æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def load_dataset_from_disk(
    dataset_path: str,
    dataset_name: str = "winogrande"
) -> Any:
    """
    ä»æœ¬åœ°ç£ç›˜åŠ è½½æ•°æ®é›†
    
    Args:
        dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
        dataset_name: æ•°æ®é›†åç§°
    
    Returns:
        åŠ è½½çš„æ•°æ®é›†
    """
    try:
        full_path = os.path.join(dataset_path, dataset_name)
        print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ•°æ®é›†: {full_path}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(full_path):
            raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {full_path}")
        
        # ä»ç£ç›˜åŠ è½½æ•°æ®é›†
        dataset = load_from_disk(full_path)
        
        print(f"æˆåŠŸä»æœ¬åœ°åŠ è½½æ•°æ®é›†ï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        print(f"æ•°æ®é›†ç‰¹å¾: {dataset.features}")
        
        return dataset
        
    except Exception as e:
        print(f"ä»æœ¬åœ°åŠ è½½æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def process_wmdp_dataset(dataset: Any, max_tokens: int = 5000) -> Any:
    """
    å¤„ç†WMDPæ•°æ®é›†ï¼Œæ·»åŠ æ–°çš„å…ƒä¿¡æ¯ï¼Œå¹¶è¿‡æ»¤æ‰tokené•¿åº¦è¶…è¿‡é™åˆ¶çš„æ ·æœ¬
    
    Args:
        dataset: åŸå§‹æ•°æ®é›†
        bio_dataset: bioæ•°æ®é›†ï¼Œç”¨äºç”ŸæˆchoiceE
        max_tokens: æœ€å¤§tokenæ•°é‡é™åˆ¶ï¼Œè¶…è¿‡æ­¤æ•°é‡çš„æ ·æœ¬å°†è¢«è¿‡æ»¤æ‰
    
    Returns:
        å¤„ç†åçš„æ•°æ®é›†
    """
    import random
    
    # åŠ è½½tokenizerç”¨äºæ£€æŸ¥tokené•¿åº¦
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
    print(f"ä½¿ç”¨tokenizeræ£€æŸ¥tokené•¿åº¦ï¼Œæœ€å¤§é™åˆ¶: {max_tokens}")
    
    def process_sample(sample):
        # è·å–åŸå§‹æ•°æ®
        label = sample['answer']
        question = sample['sentence']
        answer = sample['option1'] if label == 1 else sample['option2']
        false_answer = sample['option2'] if label == 1 else sample['option1']
        #choices = sample['choices']
        sentence=(f"{question}"
                  f"\nShould the '_' be " f"{answer}?"
                  f"\nAnswer: Yes")
        corr_sentence=(f"{question}"
                  f"\nThe '_' should not be " f"{false_answer}.")
        

        
        # æ£€æŸ¥tokené•¿åº¦
        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)
        corr_sentence_tokens = tokenizer.encode(corr_sentence, add_special_tokens=False)
        
        sentence_token_count = len(sentence_tokens)
        corr_sentence_token_count = len(corr_sentence_tokens)
        
        # å¦‚æœä»»ä¸€æ–‡æœ¬è¶…è¿‡tokené™åˆ¶ï¼Œè¿”å›Noneè¡¨ç¤ºè¿‡æ»¤æ‰
        if sentence_token_count > max_tokens or corr_sentence_token_count > max_tokens or sentence_token_count<23:
            return None
        
        # è¿”å›åŒ…å«æ‰€æœ‰å­—æ®µçš„æ ·æœ¬
        return {
            'answer': answer,
            'question': question,
            'sentence': sentence,
            'corr_sentence': corr_sentence,
            'sentence_token_count': sentence_token_count,
            'corr_sentence_token_count': corr_sentence_token_count
        }
    
    # å¤„ç†æ‰€æœ‰æ ·æœ¬å¹¶è¿‡æ»¤
    processed_samples = []
    filtered_count = 0
    
    for i, sample in enumerate(dataset):
        if i % 100 == 0:  # æ¯å¤„ç†100ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
            print(f"æ­£åœ¨å¤„ç†æ ·æœ¬ {i+1}/{len(dataset)}")
        
        processed_sample = process_sample(sample)
        if processed_sample is not None:
            processed_samples.append(processed_sample)
        else:
            filtered_count += 1
    
    # åˆ›å»ºæ–°çš„æ•°æ®é›†
    from datasets import Dataset
    processed_dataset = Dataset.from_list(processed_samples)
    
    print(f"æ•°æ®é›†å¤„ç†å®Œæˆï¼ŒåŒ…å« {len(processed_dataset)} ä¸ªæ ·æœ¬")
    print(f"è¿‡æ»¤æ‰çš„æ ·æœ¬æ•°: {filtered_count} (tokené•¿åº¦è¶…è¿‡{max_tokens})")
    print(f"ä¿ç•™ç‡: {len(processed_dataset)/len(dataset)*100:.2f}%")
    print(f"æ–°çš„æ•°æ®é›†ç‰¹å¾: {processed_dataset.features}")
    
    return processed_dataset

def validate_samples_with_llm(dataset: Any, num_samples: int = None) -> Any:
    """
    ä½¿ç”¨LLMéªŒè¯æ•°æ®é›†æ ·æœ¬çš„è¾“å…¥æ˜¯å¦æ­£ç¡®ï¼Œå¹¶ç»Ÿè®¡å‡†ç¡®ç‡
    
    Args:
        dataset: è¦éªŒè¯çš„æ•°æ®é›†
        num_samples: è¦éªŒè¯çš„æ ·æœ¬æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™éªŒè¯æ‰€æœ‰æ ·æœ¬
    """
    try:
        print(f"æ­£åœ¨åŠ è½½LLMæ¨¡å‹è¿›è¡ŒéªŒè¯...")
        
        # å¯¼å…¥æ¨¡å‹
        sys.path.append(os.path.join(os.getcwd(), "src/modeling/"))
        from transformers import MistralForCausalLM
        from transformers import AutoTokenizer
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        model = MistralForCausalLM.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
        tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
        tokenizer.pad_token = tokenizer.eos_token
        
        # å°†æ¨¡å‹ç§»åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        model.eval()
        
        print(f"æ¨¡å‹å·²åŠ è½½åˆ° {device}")
        
        # ç¡®å®šè¦éªŒè¯çš„æ ·æœ¬æ•°é‡
        total_samples = len(dataset)
        if num_samples is None:
            num_samples = total_samples
        else:
            num_samples = min(num_samples, total_samples)
        
        print(f"å¼€å§‹éªŒè¯ {num_samples} ä¸ªæ ·æœ¬...")
        
        # ç»Ÿè®¡å˜é‡
        sentence_correct = 0
        corr_sentence_correct = 0
        total_validated = 0
        
        # Tokenæ•°é‡ç»Ÿè®¡å˜é‡
        max_sentence_tokens = 0
        max_corr_sentence_tokens = 0
        sentence_token_counts = []
        corr_sentence_token_counts = []
        
        # ä¿å­˜æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬
        correct_samples = []
        
        # éªŒè¯æ ·æœ¬
        for i in range(num_samples):
            sample = dataset[i]
            
            # è·å–sentenceå’Œcorr_sentence
            sentence = sample['sentence']
            corr_sentence = sample['corr_sentence']
            
            print(f"\n{'='*60}")
            print(f"éªŒè¯æ ·æœ¬ {i+1}/{num_samples}")
            print(f"{'='*60}")
            
            # éªŒè¯sentence
            print(f"\nåŸå§‹sentence:")
            print(f"å®Œæ•´æ–‡æœ¬: {sentence}")
            
            # ç»Ÿè®¡sentenceçš„tokenæ•°é‡
            sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)
            sentence_token_count = len(sentence_tokens)
            sentence_token_counts.append(sentence_token_count)
            if sentence_token_count > max_sentence_tokens:
                max_sentence_tokens = sentence_token_count
            
            print(f"Tokenæ•°é‡: {sentence_token_count}")
            
            sentence_prediction_correct = False
            # åˆ†å‰²è¾“å…¥å’Œæ ‡ç­¾
            last_space_idx = sentence.rfind(' ')
            if last_space_idx != -1:
                input_text = sentence[:last_space_idx]
                target_label = sentence[last_space_idx:]  # åŒ…å«ç©ºæ ¼
                
                print(f"è¾“å…¥éƒ¨åˆ†: {input_text}")
                print(f"ç›®æ ‡æ ‡ç­¾: {target_label}")
                
                # ä½¿ç”¨LLMç”Ÿæˆé¢„æµ‹
                with torch.no_grad():
                    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=1,
                        do_sample=False,
                        temperature=0.0,
                        pad_token_id=tokenizer.eos_token_id
                    )
                    
                    # è§£ç ç”Ÿæˆçš„token
                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    predicted_token = tokenizer.decode(outputs[0][-1:], skip_special_tokens=True)
                    
                    print(f"æ¨¡å‹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtoken: '{predicted_token}'")
                    print(f"æœŸæœ›çš„æ ‡ç­¾: '{target_label}'")
                    
                    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®
                    if predicted_token.strip() == target_label.strip():
                        print("âœ… é¢„æµ‹æ­£ç¡®!")
                        sentence_prediction_correct = True
                        sentence_correct += 1
                    else:
                        print("âŒ é¢„æµ‹é”™è¯¯!")
            else:
                print("æ— æ³•æ‰¾åˆ°æœ€åä¸€ä¸ªç©ºæ ¼æ¥åˆ†å‰²è¾“å…¥å’Œæ ‡ç­¾")
            
            # éªŒè¯corr_sentence
            print(f"\nå¹²æ‰°sentence:")
            print(f"å®Œæ•´æ–‡æœ¬: {corr_sentence}")
            
            # ç»Ÿè®¡corr_sentenceçš„tokenæ•°é‡
            corr_sentence_tokens = tokenizer.encode(corr_sentence, add_special_tokens=False)
            corr_sentence_token_count = len(corr_sentence_tokens)
            corr_sentence_token_counts.append(corr_sentence_token_count)
            if corr_sentence_token_count > max_corr_sentence_tokens:
                max_corr_sentence_tokens = corr_sentence_token_count
            
            print(f"Tokenæ•°é‡: {corr_sentence_token_count}")
            
            corr_sentence_prediction_correct = False
            # åˆ†å‰²è¾“å…¥å’Œæ ‡ç­¾
            last_space_idx = corr_sentence.rfind(' ')
            if last_space_idx != -1:
                input_text = corr_sentence[:last_space_idx]
                target_label = corr_sentence[last_space_idx+1:]  # åŒ…å«ç©ºæ ¼
                
                print(f"è¾“å…¥éƒ¨åˆ†: {input_text}")
                print(f"ç›®æ ‡æ ‡ç­¾: {target_label}")
                
                # ä½¿ç”¨LLMç”Ÿæˆé¢„æµ‹
                with torch.no_grad():
                    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=1,
                        do_sample=False,
                        temperature=0.0,
                        pad_token_id=tokenizer.eos_token_id
                    )
                    
                    # è§£ç ç”Ÿæˆçš„token
                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    predicted_token = tokenizer.decode(outputs[0][-1:], skip_special_tokens=True)
                    
                    print(f"æ¨¡å‹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtoken: '{predicted_token}'")
                    print(f"æœŸæœ›çš„æ ‡ç­¾: '{target_label}'")
                    
                    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®
                    if predicted_token.strip() == target_label.strip():
                        print("âœ… é¢„æµ‹æ­£ç¡®!")
                        corr_sentence_prediction_correct = True
                        corr_sentence_correct += 1
                    else:
                        print("âŒ é¢„æµ‹é”™è¯¯!")
            else:
                print("æ— æ³•æ‰¾åˆ°æœ€åä¸€ä¸ªç©ºæ ¼æ¥åˆ†å‰²è¾“å…¥å’Œæ ‡ç­¾")
            
            # æ›´æ–°æ€»éªŒè¯æ•°
            total_validated += 1
            
            # æ˜¾ç¤ºå½“å‰æ ·æœ¬çš„éªŒè¯ç»“æœ
            print(f"\næ ·æœ¬ {i+1} éªŒè¯ç»“æœ:")
            print(f"  sentenceé¢„æµ‹: {'âœ… æ­£ç¡®' if sentence_prediction_correct else 'âŒ é”™è¯¯'}")
            print(f"  corr_sentenceé¢„æµ‹: {'âœ… æ­£ç¡®' if corr_sentence_prediction_correct else 'âŒ é”™è¯¯'}")
            
            # å¦‚æœsentenceå’Œcorr_sentenceéƒ½é¢„æµ‹æ­£ç¡®ï¼Œä¿å­˜è¯¥æ ·æœ¬
            if sentence_prediction_correct:
                correct_samples.append(sample)
                print(f"  ğŸ¯ è¯¥æ ·æœ¬sentenceå’Œcorr_sentenceéƒ½é¢„æµ‹æ­£ç¡®ï¼Œå·²ä¿å­˜")
            if len(correct_samples)>200:
                break
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ç»“æœ
        sentence_accuracy = (sentence_correct / total_validated) * 100 if total_validated > 0 else 0
        corr_sentence_accuracy = (corr_sentence_correct / total_validated) * 100 if total_validated > 0 else 0
        
        # è®¡ç®—tokenç»Ÿè®¡ä¿¡æ¯
        avg_sentence_tokens = sum(sentence_token_counts) / len(sentence_token_counts) if sentence_token_counts else 0
        avg_corr_sentence_tokens = sum(corr_sentence_token_counts) / len(corr_sentence_token_counts) if corr_sentence_token_counts else 0
        
        print(f"\n{'='*60}")
        print(f"LLMéªŒè¯å®Œæˆï¼Œå…±éªŒè¯äº† {total_validated} ä¸ªæ ·æœ¬")
        print(f"{'='*60}")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ç»“æœ:")
        print(f"  sentenceå‡†ç¡®ç‡: {sentence_correct}/{total_validated} = {sentence_accuracy:.2f}%")
        print(f"  corr_sentenceå‡†ç¡®ç‡: {corr_sentence_correct}/{total_validated} = {corr_sentence_accuracy:.2f}%")
        print(f"  sentenceæ­£ç¡®é¢„æµ‹æ•°: {sentence_correct}")
        print(f"  corr_sentenceæ­£ç¡®é¢„æµ‹æ•°: {corr_sentence_correct}")
        print(f"  corr_sentenceé”™è¯¯é¢„æµ‹æ•°: {total_validated - corr_sentence_correct}")
        print(f"\nğŸ”¢ Tokenæ•°é‡ç»Ÿè®¡:")
        print(f"  sentenceæœ€å¤§tokenæ•°: {max_sentence_tokens}")
        print(f"  corr_sentenceæœ€å¤§tokenæ•°: {max_corr_sentence_tokens}")
        print(f"  sentenceå¹³å‡tokenæ•°: {avg_sentence_tokens:.2f}")
        print(f"  corr_sentenceå¹³å‡tokenæ•°: {avg_corr_sentence_tokens:.2f}")
        print(f"  sentence tokenæ•°èŒƒå›´: {min(sentence_token_counts)} - {max_sentence_tokens}")
        print(f"  corr_sentence tokenæ•°èŒƒå›´: {min(corr_sentence_token_counts)} - {max_corr_sentence_tokens}")
        print(f"\nğŸ“ æ­£ç¡®é¢„æµ‹æ ·æœ¬ç»Ÿè®¡:")
        print(f"  sentenceå’Œcorr_sentenceéƒ½æ­£ç¡®çš„æ ·æœ¬æ•°: {len(correct_samples)}")
        print(f"  {len(correct_samples)}/{total_validated} = {(len(correct_samples)/total_validated)*100:.2f}%")
        print(f"{'='*60}")
        
        # ä¿å­˜æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬ä¸ºæ–°çš„æ•°æ®é›†
        if correct_samples:
            try:
                from datasets import Dataset
                correct_dataset = Dataset.from_list(correct_samples)
                
                # ä¿å­˜åˆ°æœ¬åœ°
                save_path = "./data/datasets/winogrande"
                correct_dataset.save_to_disk(save_path)
                
                print(f"\nğŸ’¾ æ­£ç¡®é¢„æµ‹æ ·æœ¬æ•°æ®é›†å·²ä¿å­˜åˆ°: {save_path}")
                print(f"   æ•°æ®é›†å¤§å°: {len(correct_dataset)} ä¸ªæ ·æœ¬")
                print(f"   æ•°æ®é›†ç‰¹å¾: {correct_dataset.features}")
                
                return correct_dataset
            except Exception as e:
                print(f"ä¿å­˜æ­£ç¡®é¢„æµ‹æ ·æœ¬æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return None
        else:
            print(f"\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°sentenceå’Œcorr_sentenceéƒ½é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬")
            return None
        
    except Exception as e:
        print(f"LLMéªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def explore_dataset(dataset: Any, num_samples: int = 5) -> None:
    """
    æ¢ç´¢æ•°æ®é›†çš„ç»“æ„å’Œå†…å®¹
    
    Args:
        dataset: åŠ è½½çš„æ•°æ®é›†
        num_samples: è¦æ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡
    """
    print("=" * 50)
    print("æ•°æ®é›†æ¢ç´¢")
    print("=" * 50)
    
    # æ˜¾ç¤ºæ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"æ•°æ®é›†ç‰¹å¾: {dataset.features}")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬
    print(f"\nå‰ {num_samples} ä¸ªæ ·æœ¬:")
    for i, sample in enumerate(dataset.select(range(min(num_samples, len(dataset))))):
        print(f"\næ ·æœ¬ {i+1}:")
        for key, value in sample.items():
            print(f"  {key}: {value}")

def main():
    """
    ä¸»å‡½æ•°
    """
    try:
        # åŠ è½½WMDP cyberæ•°æ®é›†
        print("="*50)
        print("åŠ è½½WMDP cyberæ•°æ®é›†")
        print("="*50)
        dataset = load_wmdp_dataset()
        
        # åŠ è½½WMDP bioæ•°æ®é›†ç”¨äºç”ŸæˆchoiceE
        print("\n" + "="*50)
        
        # æ¢ç´¢åŸå§‹æ•°æ®é›†
        print("\n" + "="*50)
        print("åŸå§‹æ•°æ®é›†æ¢ç´¢")
        print("="*50)
        explore_dataset(dataset)
        
        # å¤„ç†æ•°æ®é›†ï¼Œæ·»åŠ æ–°çš„å…ƒä¿¡æ¯
        print("\n" + "="*50)
        print("å¤„ç†æ•°æ®é›†ï¼Œæ·»åŠ æ–°çš„å…ƒä¿¡æ¯")
        print("="*50)
        processed_dataset = process_wmdp_dataset(dataset, max_tokens=5000)
        
        # æ¢ç´¢å¤„ç†åçš„æ•°æ®é›†
        print("\n" + "="*50)
        print("å¤„ç†åæ•°æ®é›†æ¢ç´¢")
        print("="*50)
        explore_dataset(processed_dataset)
        
        # ä½¿ç”¨LLMéªŒè¯æ ·æœ¬
        print("\n" + "="*50)
        print("ä½¿ç”¨LLMéªŒè¯æ ·æœ¬")
        print("="*50)
        processed_dataset = validate_samples_with_llm(processed_dataset, num_samples=None)  # éªŒè¯æ‰€æœ‰æ ·æœ¬
        
        # # å¤„ç†æ­£ç¡®é¢„æµ‹çš„æ•°æ®é›†
        # if correct_dataset is not None:
        #     print("\n" + "="*50)
        #     print("æ­£ç¡®é¢„æµ‹æ ·æœ¬æ•°æ®é›†ä¿¡æ¯")
        #     print("="*50)
        #     print(f"æ•°æ®é›†å¤§å°: {len(correct_dataset)} ä¸ªæ ·æœ¬")
        #     print(f"æ•°æ®é›†ç‰¹å¾: {correct_dataset.features}")
            
        #     # æ¢ç´¢æ­£ç¡®é¢„æµ‹çš„æ•°æ®é›†
        #     if len(correct_dataset) > 0:
        #         print(f"\nå‰3ä¸ªæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬:")
        #         for i, sample in enumerate(correct_dataset.select(range(min(3, len(correct_dataset))))):
        #             print(f"\næ ·æœ¬ {i+1}:")
        #             for key, value in sample.items():
        #                 print(f"  {key}: {value}")
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®é›†åˆ°æœ¬åœ°ç£ç›˜
        print("\n" + "="*50)
        print("ä¿å­˜å¤„ç†åçš„æ•°æ®é›†")
        print("="*50)
        
        # åˆ’åˆ†æ•°æ®é›†ï¼š1000ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒï¼Œå‰©ä½™ç”¨äºéªŒè¯
        total_samples = len(processed_dataset)
        train_size = min(1000, total_samples)
        validation_size = min(600, total_samples - train_size)
        
        print(f"æ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {train_size}")
        print(f"éªŒè¯é›†æ ·æœ¬æ•°: {validation_size}")
        
        # åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_dataset = processed_dataset.select(range(train_size))
        validation_dataset = processed_dataset.select(range(train_size, total_samples))
        
        # åˆ›å»ºä¸»æ•°æ®é›†æ–‡ä»¶å¤¹
        import os
        main_dataset_path = "./Edge-Pruning/data/datasets/winogrande"
        os.makedirs(main_dataset_path, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒé›†åˆ°trainå­æ–‡ä»¶å¤¹
        print(f"\nä¿å­˜è®­ç»ƒé›†...")
        train_path = os.path.join(main_dataset_path, "train")
        train_dataset.save_to_disk(train_path)
        print(f"è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_path}")
        
        # ä¿å­˜éªŒè¯é›†åˆ°validationå­æ–‡ä»¶å¤¹
        if validation_size > 0:
            print(f"ä¿å­˜éªŒè¯é›†...")
            validation_path = os.path.join(main_dataset_path, "validation")
            validation_dataset.save_to_disk(validation_path)
            print(f"éªŒè¯é›†å·²ä¿å­˜åˆ°: {validation_path}")
        else:
            print(f"éªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
        
        # æ¼”ç¤ºä»æœ¬åœ°åŠ è½½å¤„ç†åçš„æ•°æ®é›†
        print("\n" + "="*50)
        print("æ¼”ç¤ºä»æœ¬åœ°åŠ è½½å¤„ç†åçš„æ•°æ®é›†")
        print("="*50)
        
        # åŠ è½½è®­ç»ƒé›†
        train_path = "./Edge-Pruning/data/datasets/winogrande/train"
        if os.path.exists(train_path):
            from datasets import load_from_disk
            local_train_dataset = load_from_disk(train_path)
            print(f"è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(local_train_dataset)} ä¸ªæ ·æœ¬")
        else:
            print("è®­ç»ƒé›†æ–‡ä»¶ä¸å­˜åœ¨")
            local_train_dataset = None
        
        # åŠ è½½éªŒè¯é›†
        validation_path = "./Edge-Pruning/data/datasets/winogrande/validation"
        if os.path.exists(validation_path):
            local_validation_dataset = load_from_disk(validation_path)
            print(f"éªŒè¯é›†åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(local_validation_dataset)} ä¸ªæ ·æœ¬")
        else:
            print("éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨")
            local_validation_dataset = None
        
        # æµ‹è¯•tokené•¿åº¦
        print("\n" + "="*50)
        print("æµ‹è¯•æ•°æ®é›†tokené•¿åº¦")
        print("="*50)
        
        # åŠ è½½tokenizer
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
        print("TokenizeråŠ è½½æˆåŠŸ")
        
        def analyze_token_lengths(dataset, dataset_name):
            """åˆ†ææ•°æ®é›†çš„tokené•¿åº¦"""
            if dataset is None:
                print(f"{dataset_name}æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ†æ")
                return
            
            print(f"\nåˆ†æ{dataset_name}æ•°æ®é›†...")
            
            # ç»Ÿè®¡å˜é‡
            sentence_token_lengths = []
            corr_sentence_token_lengths = []
            max_sentence_tokens = 0
            max_corr_sentence_tokens = 0
            max_sentence_sample = None
            max_corr_sentence_sample = None
            over_limit_count = 0
            
            # åˆ†ææ¯ä¸ªæ ·æœ¬
            for i, sample in enumerate(dataset):
                # åˆ†æsentence
                sentence = sample['sentence']
                sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)
                sentence_token_count = len(sentence_tokens)
                sentence_token_lengths.append(sentence_token_count)
                
                if sentence_token_count > max_sentence_tokens:
                    max_sentence_tokens = sentence_token_count
                    max_sentence_sample = sample
                
                # åˆ†æcorr_sentence
                corr_sentence = sample['corr_sentence']
                corr_sentence_tokens = tokenizer.encode(corr_sentence, add_special_tokens=False)
                corr_sentence_token_count = len(corr_sentence_tokens)
                corr_sentence_token_lengths.append(corr_sentence_token_count)
                
                if corr_sentence_token_count > max_corr_sentence_tokens:
                    max_corr_sentence_tokens = corr_sentence_token_count
                    max_corr_sentence_sample = sample
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶…è¿‡1000çš„æ ·æœ¬
                if sentence_token_count > 30 or corr_sentence_token_count > 30:
                    over_limit_count += 1
                    print(f"  è­¦å‘Š: æ ·æœ¬ {i} è¶…è¿‡1000 tokens - sentence: {sentence_token_count}, corr_sentence: {corr_sentence_token_count}")
                
                # æ¯100ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 100 == 0:
                    print(f"  å·²å¤„ç† {i + 1}/{len(dataset)} ä¸ªæ ·æœ¬")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_sentence_tokens = sum(sentence_token_lengths) / len(sentence_token_lengths)
            avg_corr_sentence_tokens = sum(corr_sentence_token_lengths) / len(corr_sentence_token_lengths)
            min_sentence_tokens = min(sentence_token_lengths)
            min_corr_sentence_tokens = min(corr_sentence_token_lengths)
            
            # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
            print(f"\n{dataset_name}æ•°æ®é›†Tokené•¿åº¦ç»Ÿè®¡:")
            print(f"  sentence:")
            print(f"    æœ€å¤§tokenæ•°: {max_sentence_tokens}")
            print(f"    æœ€å°tokenæ•°: {min_sentence_tokens}")
            print(f"    å¹³å‡tokenæ•°: {avg_sentence_tokens:.2f}")
            print(f"    tokenæ•°èŒƒå›´: {min_sentence_tokens} - {max_sentence_tokens}")
            
            print(f"  corr_sentence:")
            print(f"    æœ€å¤§tokenæ•°: {max_corr_sentence_tokens}")
            print(f"    æœ€å°tokenæ•°: {min_corr_sentence_tokens}")
            print(f"    å¹³å‡tokenæ•°: {avg_corr_sentence_tokens:.2f}")
            print(f"    tokenæ•°èŒƒå›´: {min_corr_sentence_tokens} - {max_corr_sentence_tokens}")
            
            if over_limit_count > 0:
                print(f"  âš ï¸  è­¦å‘Š: å‘ç° {over_limit_count} ä¸ªæ ·æœ¬è¶…è¿‡1000 tokensé™åˆ¶")
            else:
                print(f"  âœ… æ‰€æœ‰æ ·æœ¬éƒ½åœ¨1000 tokensé™åˆ¶å†…")
            
            # æ˜¾ç¤ºæœ€é•¿æ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            if max_sentence_sample:
                print(f"\næœ€é•¿sentenceæ ·æœ¬ (tokenæ•°: {max_sentence_tokens}):")
                print(f"  sentence: {max_sentence_sample['sentence']}")
                print(f"  corr_sentence: {max_sentence_sample['corr_sentence']}")
            
            if max_corr_sentence_sample and max_corr_sentence_sample != max_sentence_sample:
                print(f"\næœ€é•¿corr_sentenceæ ·æœ¬ (tokenæ•°: {max_corr_sentence_tokens}):")
                print(f"  sentence: {max_corr_sentence_sample['sentence']}")
                print(f"  corr_sentence: {max_corr_sentence_sample['corr_sentence']}")
            
            return {
                'max_sentence_tokens': max_sentence_tokens,
                'max_corr_sentence_tokens': max_corr_sentence_tokens,
                'avg_sentence_tokens': avg_sentence_tokens,
                'avg_corr_sentence_tokens': avg_corr_sentence_tokens
            }
        
        # åˆ†æè®­ç»ƒé›†
        train_stats = analyze_token_lengths(local_train_dataset, "è®­ç»ƒé›†")
        
        # åˆ†æéªŒè¯é›†
        validation_stats = analyze_token_lengths(local_validation_dataset, "éªŒè¯é›†")
        
        # ç»¼åˆç»Ÿè®¡
        if train_stats and validation_stats:
            print(f"\n{'='*60}")
            print("ç»¼åˆç»Ÿè®¡ç»“æœ:")
            print(f"{'='*60}")
            print(f"è®­ç»ƒé›† + éªŒè¯é›†æœ€å¤§tokenæ•°:")
            print(f"  sentence: {max(train_stats['max_sentence_tokens'], validation_stats['max_sentence_tokens'])}")
            print(f"  corr_sentence: {max(train_stats['max_corr_sentence_tokens'], validation_stats['max_corr_sentence_tokens'])}")
            print(f"è®­ç»ƒé›† + éªŒè¯é›†å¹³å‡tokenæ•°:")
            print(f"  sentence: {(train_stats['avg_sentence_tokens'] + validation_stats['avg_sentence_tokens']) / 2:.2f}")
            print(f"  corr_sentence: {(train_stats['avg_corr_sentence_tokens'] + validation_stats['avg_corr_sentence_tokens']) / 2:.2f}")
        
        # è¿”å›å¤„ç†åçš„æ•°æ®é›†ä¾›è¿›ä¸€æ­¥å¤„ç†
        return processed_dataset
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    dataset = main()
    if dataset is not None:
        print(f"\næ•°æ®é›†åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
    else:
        print("æ•°æ®é›†åŠ è½½å¤±è´¥ï¼")
