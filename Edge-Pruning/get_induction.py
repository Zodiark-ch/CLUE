#!/usr/bin/env python
# coding=utf-8
"""
WMDPæ•°æ®é›†åŠ è½½å’Œå¤„ç†è„šæœ¬
ç”¨äºé‡æ–°å¤„ç†WMDPæ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯cyberå­é›†
"""

import os
import sys
import torch
from datasets import load_dataset, load_from_disk
from typing import Optional, Dict, Any
import huggingface_hub
def load_wmdp_dataset(
    dataset_name: str = "cais/wmdp",
    subset: str = "wmdp-bio",
    split: str = "test",
    cache_dir: str = "./.cache"
) -> Any:
    """
    åŠ è½½WMDPæ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°ï¼Œé»˜è®¤ä¸º "cais/wmdp"
        subset: æ•°æ®é›†å­é›†ï¼Œé»˜è®¤ä¸º "wmdp-cyber"
        split: æ•°æ®é›†åˆ†å‰²ï¼Œé»˜è®¤ä¸º "test"
        cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º "./.cache"
    
    Returns:
        åŠ è½½çš„æ•°æ®é›†
    """
    try:
        print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name}, å­é›†: {subset}, åˆ†å‰²: {split}")
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset(
            dataset_name, 
            subset, 
            cache_dir=cache_dir
        )[split]
        
        print(f"æˆåŠŸåŠ è½½æ•°æ®é›†ï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        print(f"æ•°æ®é›†ç‰¹å¾: {dataset.features}")
        
        return dataset
        
    except Exception as e:
        print(f"åŠ è½½æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def get_validation_data(num_examples=None, seq_len=None):
    validation_fname = huggingface_hub.hf_hub_download(
        repo_id="ArthurConmy/redwood_attn_2l", filename="validation_data.pt"
    )
    validation_data = torch.load(validation_fname).long()

    if num_examples is None:
        return validation_data
    else:
        return validation_data[:num_examples][:seq_len]


def save_dataset_to_disk(
    dataset: Any,
    save_path: str,
    dataset_name: str = "wmdp-bio"
) -> None:
    """
    å°†æ•°æ®é›†ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨load_from_diskåŠ è½½
    
    Args:
        dataset: è¦ä¿å­˜çš„æ•°æ®é›†
        save_path: ä¿å­˜è·¯å¾„
        dataset_name: æ•°æ®é›†åç§°ï¼Œç”¨äºåˆ›å»ºç›®å½•
    """
    try:
        # åˆ›å»ºä¿å­˜ç›®å½•
        full_save_path = os.path.join(save_path, dataset_name)
        os.makedirs(full_save_path, exist_ok=True)
        
        print(f"æ­£åœ¨ä¿å­˜æ•°æ®é›†åˆ°: {full_save_path}")
        
        # ä¿å­˜æ•°æ®é›†
        dataset.save_to_disk(full_save_path)
        
        print(f"æ•°æ®é›†å·²æˆåŠŸä¿å­˜åˆ°: {full_save_path}")
        print(f"ä¿å­˜çš„æ•°æ®é›†åŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        
    except Exception as e:
        print(f"ä¿å­˜æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def load_dataset_from_disk(
    dataset_path: str,
    dataset_name: str = "wmdp-bio"
) -> Any:
    """
    ä»æœ¬åœ°ç£ç›˜åŠ è½½æ•°æ®é›†
    
    Args:
        dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
        dataset_name: æ•°æ®é›†åç§°
    
    Returns:
        åŠ è½½çš„æ•°æ®é›†
    """
    try:
        full_path = os.path.join(dataset_path, dataset_name)
        print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ•°æ®é›†: {full_path}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(full_path):
            raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {full_path}")
        
        # ä»ç£ç›˜åŠ è½½æ•°æ®é›†
        dataset = load_from_disk(full_path)
        
        print(f"æˆåŠŸä»æœ¬åœ°åŠ è½½æ•°æ®é›†ï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        print(f"æ•°æ®é›†ç‰¹å¾: {dataset.features}")
        
        return dataset
        
    except Exception as e:
        print(f"ä»æœ¬åœ°åŠ è½½æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def process_wmdp_dataset(dataset: Any, bio_dataset: Any = None, max_tokens: int = 5000) -> Any:
    """
    å¤„ç†WMDPæ•°æ®é›†ï¼Œæ·»åŠ æ–°çš„å…ƒä¿¡æ¯ï¼Œå¹¶è¿‡æ»¤æ‰tokené•¿åº¦è¶…è¿‡é™åˆ¶çš„æ ·æœ¬
    
    Args:
        dataset: åŸå§‹æ•°æ®é›†
        bio_dataset: bioæ•°æ®é›†ï¼Œç”¨äºç”ŸæˆchoiceE
        max_tokens: æœ€å¤§tokenæ•°é‡é™åˆ¶ï¼Œè¶…è¿‡æ­¤æ•°é‡çš„æ ·æœ¬å°†è¢«è¿‡æ»¤æ‰
    
    Returns:
        å¤„ç†åçš„æ•°æ®é›†
    """
    import random
    
    # åŠ è½½tokenizerç”¨äºæ£€æŸ¥tokené•¿åº¦
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
    print(f"ä½¿ç”¨tokenizeræ£€æŸ¥tokené•¿åº¦ï¼Œæœ€å¤§é™åˆ¶: {max_tokens}")
    
    def process_sample(sample):
        # è·å–åŸå§‹æ•°æ®
        answer = sample['answer']
        question = sample['question']
        choices = sample['choices']
        
        # 1. choiceAä¸ºæ­£ç¡®ç­”æ¡ˆï¼Œä»choices[answer]è·å–
        if 0 <= answer < len(choices):
            choiceA = choices[answer]
        else:
            choiceA = choices[0] if len(choices) > 0 else ""
        
        # 2. choiceBä»bioæ•°æ®é›†ä¸­éšæœºé€‰æ‹©
        if bio_dataset is not None and len(bio_dataset) > 0:
            random_bio_sample = random.choice(bio_dataset)
            random_bio_choices = random_bio_sample['choices']
            if len(random_bio_choices) > 0:
                choiceB = random.choice(random_bio_choices)
            else:
                choiceB = "No bio choice available"
        else:
            choiceB = "No bio dataset available"
        
        # 3. æ„å»ºsentence - ç°åœ¨åªæœ‰Aå’ŒBä¸¤ä¸ªé€‰é¡¹
        sentence = (
            "Please choose the answer that is most relevant to the question from options A and B. "
            f"The Question: {question} "
            f"The Options: "
            f"A. {choiceA} "
            f"B. {choiceB} "
            f"The answer is: A"
        )
        
        # 4. æ„å»ºcorr_sentence - é€‰æ‹©Bä½œä¸ºé”™è¯¯ç­”æ¡ˆ
        corr_sentence = (
            "Please choose the answer that is least relevant to the question from options A and B. "
            f"The Question: {question} "
            f"The Options: "
            f"A. {choiceA} "
            f"B. {choiceB} "
            f"The answer is: B"
        )
        
        # æ£€æŸ¥tokené•¿åº¦
        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)
        corr_sentence_tokens = tokenizer.encode(corr_sentence, add_special_tokens=False)
        
        sentence_token_count = len(sentence_tokens)
        corr_sentence_token_count = len(corr_sentence_tokens)
        
        # å¦‚æœä»»ä¸€æ–‡æœ¬è¶…è¿‡tokené™åˆ¶ï¼Œè¿”å›Noneè¡¨ç¤ºè¿‡æ»¤æ‰
        if sentence_token_count > max_tokens or corr_sentence_token_count > max_tokens:
            return None
        
        # è¿”å›åŒ…å«æ‰€æœ‰å­—æ®µçš„æ ·æœ¬
        return {
            'answer': answer,
            'question': question,
            'choices': choices,
            'choiceA': choiceA,
            'choiceB': choiceB,
            'sentence': sentence,
            'corr_sentence': corr_sentence,
            'sentence_token_count': sentence_token_count,
            'corr_sentence_token_count': corr_sentence_token_count
        }
    
    # å¤„ç†æ‰€æœ‰æ ·æœ¬å¹¶è¿‡æ»¤
    processed_samples = []
    filtered_count = 0
    
    for i, sample in enumerate(dataset):
        if i % 100 == 0:  # æ¯å¤„ç†100ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
            print(f"æ­£åœ¨å¤„ç†æ ·æœ¬ {i+1}/{len(dataset)}")
        
        processed_sample = process_sample(sample)
        if processed_sample is not None:
            processed_samples.append(processed_sample)
        else:
            filtered_count += 1
    
    # åˆ›å»ºæ–°çš„æ•°æ®é›†
    from datasets import Dataset
    processed_dataset = Dataset.from_list(processed_samples)
    
    print(f"æ•°æ®é›†å¤„ç†å®Œæˆï¼ŒåŒ…å« {len(processed_dataset)} ä¸ªæ ·æœ¬")
    print(f"è¿‡æ»¤æ‰çš„æ ·æœ¬æ•°: {filtered_count} (tokené•¿åº¦è¶…è¿‡{max_tokens})")
    print(f"ä¿ç•™ç‡: {len(processed_dataset)/len(dataset)*100:.2f}%")
    print(f"æ–°çš„æ•°æ®é›†ç‰¹å¾: {processed_dataset.features}")
    
    return processed_dataset

def validate_samples_with_llm(dataset: Any, num_samples: int = None) -> Any:
    """
    ä½¿ç”¨LLMéªŒè¯æ•°æ®é›†æ ·æœ¬çš„è¾“å…¥æ˜¯å¦æ­£ç¡®ï¼Œå¹¶ç»Ÿè®¡å‡†ç¡®ç‡
    
    Args:
        dataset: è¦éªŒè¯çš„æ•°æ®é›†
        num_samples: è¦éªŒè¯çš„æ ·æœ¬æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™éªŒè¯æ‰€æœ‰æ ·æœ¬
    """
    try:
        print(f"æ­£åœ¨åŠ è½½LLMæ¨¡å‹è¿›è¡ŒéªŒè¯...")
        
        # å¯¼å…¥æ¨¡å‹
        sys.path.append(os.path.join(os.getcwd(), "src/modeling/"))
        from transformers import MistralForCausalLM
        from transformers import AutoTokenizer
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        model = MistralForCausalLM.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
        tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
        tokenizer.pad_token = tokenizer.eos_token
        
        # å°†æ¨¡å‹ç§»åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        model.eval()
        
        print(f"æ¨¡å‹å·²åŠ è½½åˆ° {device}")
        
        # ç¡®å®šè¦éªŒè¯çš„æ ·æœ¬æ•°é‡
        total_samples = len(dataset)
        if num_samples is None:
            num_samples = total_samples
        else:
            num_samples = min(num_samples, total_samples)
        
        print(f"å¼€å§‹éªŒè¯ {num_samples} ä¸ªæ ·æœ¬...")
        
        # ç»Ÿè®¡å˜é‡
        sentence_correct = 0
        corr_sentence_correct = 0
        total_validated = 0
        
        # Tokenæ•°é‡ç»Ÿè®¡å˜é‡
        max_sentence_tokens = 0
        max_corr_sentence_tokens = 0
        sentence_token_counts = []
        corr_sentence_token_counts = []
        
        # ä¿å­˜æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬
        correct_samples = []
        
        # éªŒè¯æ ·æœ¬
        for i in range(num_samples):
            sample = dataset[i]
            
            # è·å–sentenceå’Œcorr_sentence
            sentence = sample['sentence']
            corr_sentence = sample['corr_sentence']
            
            print(f"\n{'='*60}")
            print(f"éªŒè¯æ ·æœ¬ {i+1}/{num_samples}")
            print(f"{'='*60}")
            
            # éªŒè¯sentence
            print(f"\nåŸå§‹sentence:")
            print(f"å®Œæ•´æ–‡æœ¬: {sentence}")
            
            # ç»Ÿè®¡sentenceçš„tokenæ•°é‡
            sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)
            sentence_token_count = len(sentence_tokens)
            sentence_token_counts.append(sentence_token_count)
            if sentence_token_count > max_sentence_tokens:
                max_sentence_tokens = sentence_token_count
            
            print(f"Tokenæ•°é‡: {sentence_token_count}")
            
            sentence_prediction_correct = False
            # åˆ†å‰²è¾“å…¥å’Œæ ‡ç­¾
            last_space_idx = sentence.rfind(' ')
            if last_space_idx != -1:
                input_text = sentence[:last_space_idx]
                target_label = sentence[last_space_idx:]  # åŒ…å«ç©ºæ ¼
                
                print(f"è¾“å…¥éƒ¨åˆ†: {input_text}")
                print(f"ç›®æ ‡æ ‡ç­¾: {target_label}")
                
                # ä½¿ç”¨LLMç”Ÿæˆé¢„æµ‹
                with torch.no_grad():
                    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=5000)
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=1,
                        do_sample=False,
                        temperature=0.0,
                        pad_token_id=tokenizer.eos_token_id
                    )
                    
                    # è§£ç ç”Ÿæˆçš„token
                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    predicted_token = tokenizer.decode(outputs[0][-1:], skip_special_tokens=True)
                    
                    print(f"æ¨¡å‹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtoken: '{predicted_token}'")
                    print(f"æœŸæœ›çš„æ ‡ç­¾: '{target_label}'")
                    
                    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®
                    if predicted_token.strip() == target_label.strip():
                        print("âœ… é¢„æµ‹æ­£ç¡®!")
                        sentence_prediction_correct = True
                        sentence_correct += 1
                    else:
                        print("âŒ é¢„æµ‹é”™è¯¯!")
            else:
                print("æ— æ³•æ‰¾åˆ°æœ€åä¸€ä¸ªç©ºæ ¼æ¥åˆ†å‰²è¾“å…¥å’Œæ ‡ç­¾")
            
            # éªŒè¯corr_sentence
            print(f"\nå¹²æ‰°sentence:")
            print(f"å®Œæ•´æ–‡æœ¬: {corr_sentence}")
            
            # ç»Ÿè®¡corr_sentenceçš„tokenæ•°é‡
            corr_sentence_tokens = tokenizer.encode(corr_sentence, add_special_tokens=False)
            corr_sentence_token_count = len(corr_sentence_tokens)
            corr_sentence_token_counts.append(corr_sentence_token_count)
            if corr_sentence_token_count > max_corr_sentence_tokens:
                max_corr_sentence_tokens = corr_sentence_token_count
            
            print(f"Tokenæ•°é‡: {corr_sentence_token_count}")
            
            corr_sentence_prediction_correct = False
            # åˆ†å‰²è¾“å…¥å’Œæ ‡ç­¾
            last_space_idx = corr_sentence.rfind(' ')
            if last_space_idx != -1:
                input_text = corr_sentence[:last_space_idx]
                target_label = corr_sentence[last_space_idx:]  # åŒ…å«ç©ºæ ¼
                
                print(f"è¾“å…¥éƒ¨åˆ†: {input_text}")
                print(f"ç›®æ ‡æ ‡ç­¾: {target_label}")
                
                # ä½¿ç”¨LLMç”Ÿæˆé¢„æµ‹
                with torch.no_grad():
                    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=1,
                        do_sample=False,
                        temperature=0.0,
                        pad_token_id=tokenizer.eos_token_id
                    )
                    
                    # è§£ç ç”Ÿæˆçš„token
                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    predicted_token = tokenizer.decode(outputs[0][-1:], skip_special_tokens=True)
                    
                    print(f"æ¨¡å‹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtoken: '{predicted_token}'")
                    print(f"æœŸæœ›çš„æ ‡ç­¾: '{target_label}'")
                    
                    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®
                    if predicted_token.strip() == target_label.strip():
                        print("âœ… é¢„æµ‹æ­£ç¡®!")
                        corr_sentence_prediction_correct = True
                        corr_sentence_correct += 1
                    else:
                        print("âŒ é¢„æµ‹é”™è¯¯!")
            else:
                print("æ— æ³•æ‰¾åˆ°æœ€åä¸€ä¸ªç©ºæ ¼æ¥åˆ†å‰²è¾“å…¥å’Œæ ‡ç­¾")
            
            # æ›´æ–°æ€»éªŒè¯æ•°
            total_validated += 1
            
            # æ˜¾ç¤ºå½“å‰æ ·æœ¬çš„éªŒè¯ç»“æœ
            print(f"\næ ·æœ¬ {i+1} éªŒè¯ç»“æœ:")
            print(f"  sentenceé¢„æµ‹: {'âœ… æ­£ç¡®' if sentence_prediction_correct else 'âŒ é”™è¯¯'}")
            print(f"  corr_sentenceé¢„æµ‹: {'âœ… æ­£ç¡®' if corr_sentence_prediction_correct else 'âŒ é”™è¯¯'}")
            
            # å¦‚æœsentenceå’Œcorr_sentenceéƒ½é¢„æµ‹æ­£ç¡®ï¼Œä¿å­˜è¯¥æ ·æœ¬
            if sentence_prediction_correct and corr_sentence_prediction_correct:
                correct_samples.append(sample)
                print(f"  ğŸ¯ è¯¥æ ·æœ¬sentenceå’Œcorr_sentenceéƒ½é¢„æµ‹æ­£ç¡®ï¼Œå·²ä¿å­˜")
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ç»“æœ
        sentence_accuracy = (sentence_correct / total_validated) * 100 if total_validated > 0 else 0
        corr_sentence_accuracy = (corr_sentence_correct / total_validated) * 100 if total_validated > 0 else 0
        
        # è®¡ç®—tokenç»Ÿè®¡ä¿¡æ¯
        avg_sentence_tokens = sum(sentence_token_counts) / len(sentence_token_counts) if sentence_token_counts else 0
        avg_corr_sentence_tokens = sum(corr_sentence_token_counts) / len(corr_sentence_token_counts) if corr_sentence_token_counts else 0
        
        print(f"\n{'='*60}")
        print(f"LLMéªŒè¯å®Œæˆï¼Œå…±éªŒè¯äº† {total_validated} ä¸ªæ ·æœ¬")
        print(f"{'='*60}")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ç»“æœ:")
        print(f"  sentenceå‡†ç¡®ç‡: {sentence_correct}/{total_validated} = {sentence_accuracy:.2f}%")
        print(f"  corr_sentenceå‡†ç¡®ç‡: {corr_sentence_correct}/{total_validated} = {corr_sentence_accuracy:.2f}%")
        print(f"  sentenceæ­£ç¡®é¢„æµ‹æ•°: {sentence_correct}")
        print(f"  corr_sentenceæ­£ç¡®é¢„æµ‹æ•°: {corr_sentence_correct}")
        print(f"  corr_sentenceé”™è¯¯é¢„æµ‹æ•°: {total_validated - corr_sentence_correct}")
        print(f"\nğŸ”¢ Tokenæ•°é‡ç»Ÿè®¡:")
        print(f"  sentenceæœ€å¤§tokenæ•°: {max_sentence_tokens}")
        print(f"  corr_sentenceæœ€å¤§tokenæ•°: {max_corr_sentence_tokens}")
        print(f"  sentenceå¹³å‡tokenæ•°: {avg_sentence_tokens:.2f}")
        print(f"  corr_sentenceå¹³å‡tokenæ•°: {avg_corr_sentence_tokens:.2f}")
        print(f"  sentence tokenæ•°èŒƒå›´: {min(sentence_token_counts)} - {max_sentence_tokens}")
        print(f"  corr_sentence tokenæ•°èŒƒå›´: {min(corr_sentence_token_counts)} - {max_corr_sentence_tokens}")
        print(f"\nğŸ“ æ­£ç¡®é¢„æµ‹æ ·æœ¬ç»Ÿè®¡:")
        print(f"  sentenceå’Œcorr_sentenceéƒ½æ­£ç¡®çš„æ ·æœ¬æ•°: {len(correct_samples)}")
        print(f"  {len(correct_samples)}/{total_validated} = {(len(correct_samples)/total_validated)*100:.2f}%")
        print(f"{'='*60}")
        
        # ä¿å­˜æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬ä¸ºæ–°çš„æ•°æ®é›†
        if correct_samples:
            try:
                from datasets import Dataset
                correct_dataset = Dataset.from_list(correct_samples)
                
                # ä¿å­˜åˆ°æœ¬åœ°
                save_path = "./data/datasets/wmdpcyber"
                correct_dataset.save_to_disk(save_path)
                
                print(f"\nğŸ’¾ æ­£ç¡®é¢„æµ‹æ ·æœ¬æ•°æ®é›†å·²ä¿å­˜åˆ°: {save_path}")
                print(f"   æ•°æ®é›†å¤§å°: {len(correct_dataset)} ä¸ªæ ·æœ¬")
                print(f"   æ•°æ®é›†ç‰¹å¾: {correct_dataset.features}")
                
                return correct_dataset
            except Exception as e:
                print(f"ä¿å­˜æ­£ç¡®é¢„æµ‹æ ·æœ¬æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return None
        else:
            print(f"\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°sentenceå’Œcorr_sentenceéƒ½é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬")
            return None
        
    except Exception as e:
        print(f"LLMéªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def explore_dataset(dataset: Any, num_samples: int = 5) -> None:
    """
    æ¢ç´¢æ•°æ®é›†çš„ç»“æ„å’Œå†…å®¹
    
    Args:
        dataset: åŠ è½½çš„æ•°æ®é›†
        num_samples: è¦æ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡
    """
    print("=" * 50)
    print("æ•°æ®é›†æ¢ç´¢")
    print("=" * 50)
    
    # æ˜¾ç¤ºæ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"æ•°æ®é›†ç‰¹å¾: {dataset.features}")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬
    print(f"\nå‰ {num_samples} ä¸ªæ ·æœ¬:")
    for i, sample in enumerate(dataset.select(range(min(num_samples, len(dataset))))):
        print(f"\næ ·æœ¬ {i+1}:")
        for key, value in sample.items():
            print(f"  {key}: {value}")

def main():
    """
    ä¸»å‡½æ•°
    """
    try:
        # åŠ è½½WMDP cyberæ•°æ®é›†
        print("="*50)
        print("åŠ è½½inductionæ•°æ®é›†")
        print("="*50)
        induction_dataset=get_validation_data(num_examples=3000)
        print(f"Inductionæ•°æ®é›†å½¢çŠ¶: {induction_dataset.shape}")
        
        from transformers import GPT2TokenizerFast
        gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('ArthurConmy/redwood_tokenizer')
        
        # å°†tensorè½¬æ¢ä¸ºtext
        print("\n" + "="*50)
        print("å°†tensorè½¬æ¢ä¸ºtext")
        print("="*50)
        
        def tensor_to_text(tensor_data, tokenizer):
            """å°†tensoræ•°æ®è½¬æ¢ä¸ºtextåˆ—è¡¨"""
            text_list = []
            
            for i in range(tensor_data.shape[0]):
                # è·å–ç¬¬iä¸ªæ ·æœ¬çš„tokenåºåˆ—
                token_sequence = tensor_data[i].tolist()
                
                # ä½¿ç”¨tokenizerè§£ç 
                try:
                    text = tokenizer.decode(token_sequence, skip_special_tokens=True)
                    text_list.append(text)
                except Exception as e:
                    print(f"è§£ç æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                    text_list.append("")  # å¦‚æœè§£ç å¤±è´¥ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                
                # æ¯å¤„ç†100ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 100 == 0:
                    print(f"  å·²å¤„ç† {i + 1}/{tensor_data.shape[0]} ä¸ªæ ·æœ¬")
            
            return text_list
        
        # è½¬æ¢tensorä¸ºtext
        induction_texts = tensor_to_text(induction_dataset, gpt2_tokenizer)
        
        print(f"è½¬æ¢å®Œæˆï¼Œå…±ç”Ÿæˆ {len(induction_texts)} ä¸ªæ–‡æœ¬æ ·æœ¬")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„è½¬æ¢ç»“æœ
        print(f"\nå‰5ä¸ªæ ·æœ¬çš„è½¬æ¢ç»“æœ:")
        for i in range(min(5, len(induction_texts))):
            print(f"æ ·æœ¬ {i+1}:")
            print(f"  åŸå§‹tensoré•¿åº¦: {len(induction_dataset[i])}")
            print(f"  è½¬æ¢åæ–‡æœ¬: {repr(induction_texts[i])}")
            print(f"  æ–‡æœ¬é•¿åº¦: {len(induction_texts[i])}")
            print()
        
        # åˆ›å»ºåŒ…å«æ–‡æœ¬çš„æ•°æ®é›†
        from datasets import Dataset
        induction_text_dataset = Dataset.from_dict({
            'text': induction_texts,
            'original_tensor_length': [len(tensor) for tensor in induction_dataset]
        })
        
        print(f"åˆ›å»ºæ–‡æœ¬æ•°æ®é›†å®Œæˆï¼ŒåŒ…å« {len(induction_text_dataset)} ä¸ªæ ·æœ¬")
        print(f"æ•°æ®é›†ç‰¹å¾: {induction_text_dataset.features}")
        
        # å¤„ç†inductionæ–‡æœ¬æ•°æ®
        print("\n" + "="*50)
        print("å¤„ç†inductionæ–‡æœ¬æ•°æ®")
        print("="*50)
        
                
            
        
        # å¤„ç†æ‰€æœ‰æ–‡æœ¬
        processed_samples = []
        filtered_count = 0
        
        for i, text in enumerate(induction_texts):
            if i % 100 == 0:
                print(f"æ­£åœ¨å¤„ç†æ ·æœ¬ {i+1}/{len(induction_texts)}")
            
            processed_sample = text
            if processed_sample is not None:
                processed_samples.append(processed_sample)
            else:
                filtered_count += 1
        
        print(f"æ–‡æœ¬å¤„ç†å®Œæˆï¼Œä¿ç•™ {len(processed_samples)} ä¸ªæ ·æœ¬ï¼Œè¿‡æ»¤ {filtered_count} ä¸ªæ ·æœ¬")
        
        # åˆ›å»ºå¤„ç†åçš„æ•°æ®é›†
        processed_induction_dataset = Dataset.from_list(processed_samples)
        print(f"å¤„ç†åæ•°æ®é›†ç‰¹å¾: {processed_induction_dataset.features}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªå¤„ç†åçš„æ ·æœ¬
        print(f"\nå‰3ä¸ªå¤„ç†åçš„æ ·æœ¬:")
        for i in range(min(3, len(processed_samples))):
            print(f"æ ·æœ¬ {i+1}:")
            print(f"  sentence: {processed_samples[i]['sentence']}")
            print(f"  corr_sentence: {processed_samples[i]['corr_sentence']}")
            print(f"  object: {processed_samples[i]['object']}")
            print()
        
        # Tokené•¿åº¦æ£€æµ‹å’Œè¿‡æ»¤
        print("\n" + "="*50)
        print("Tokené•¿åº¦æ£€æµ‹å’Œè¿‡æ»¤")
        print("="*50)
        
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
        print("TokenizeråŠ è½½æˆåŠŸ")
        
        def check_token_length(sample, max_tokens=5000):
            """æ£€æŸ¥æ ·æœ¬çš„tokené•¿åº¦"""
            sentence_tokens = tokenizer.encode(sample['sentence'], add_special_tokens=False)
            corr_sentence_tokens = tokenizer.encode(sample['corr_sentence'], add_special_tokens=False)
            
            sentence_token_count = len(sentence_tokens)
            corr_sentence_token_count = len(corr_sentence_tokens)
            
            return sentence_token_count <= max_tokens and corr_sentence_token_count <= max_tokens
        
        token_filtered_samples = []
        token_filtered_count = 0
        
        for i, sample in enumerate(processed_samples):
            if check_token_length(sample, max_tokens=5000):
                token_filtered_samples.append(sample)
            else:
                token_filtered_count += 1
            
            if (i + 1) % 100 == 0:
                print(f"  å·²æ£€æŸ¥ {i + 1}/{len(processed_samples)} ä¸ªæ ·æœ¬")
        
        print(f"Tokené•¿åº¦è¿‡æ»¤å®Œæˆï¼Œä¿ç•™ {len(token_filtered_samples)} ä¸ªæ ·æœ¬ï¼Œè¿‡æ»¤ {token_filtered_count} ä¸ªæ ·æœ¬")
        
        # åˆ›å»ºæœ€ç»ˆæ•°æ®é›†
        final_induction_dataset = Dataset.from_list(token_filtered_samples)
        print(f"æœ€ç»ˆæ•°æ®é›†å¤§å°: {len(final_induction_dataset)} ä¸ªæ ·æœ¬")
        
        # åˆ’åˆ†æ•°æ®é›†ï¼š1000ä¸ªè®­ç»ƒï¼Œ600ä¸ªéªŒè¯
        total_final_samples = len(final_induction_dataset)
        train_size = min(1000, total_final_samples)
        validation_size = min(600, total_final_samples - train_size)
        
        print(f"\næ•°æ®é›†åˆ’åˆ†:")
        print(f"  æ€»æ ·æœ¬æ•°: {total_final_samples}")
        print(f"  è®­ç»ƒé›†: {train_size}")
        print(f"  éªŒè¯é›†: {validation_size}")
        
        # åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_induction_dataset = final_induction_dataset.select(range(train_size))
        validation_induction_dataset = final_induction_dataset.select(range(train_size, train_size + validation_size))
        
        # ä¿å­˜æ•°æ®é›†
        print("\n" + "="*50)
        print("ä¿å­˜inductionæ•°æ®é›†")
        print("="*50)
        
        # åˆ›å»ºä¸»æ•°æ®é›†æ–‡ä»¶å¤¹
        import os
        main_dataset_path = "./Edge-Pruning/data/datasets/induction"
        os.makedirs(main_dataset_path, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒé›†
        print(f"ä¿å­˜è®­ç»ƒé›†...")
        train_path = os.path.join(main_dataset_path, "train")
        train_induction_dataset.save_to_disk(train_path)
        print(f"è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_path}")
        
        # ä¿å­˜éªŒè¯é›†
        if validation_size > 0:
            print(f"ä¿å­˜éªŒè¯é›†...")
            validation_path = os.path.join(main_dataset_path, "validation")
            validation_induction_dataset.save_to_disk(validation_path)
            print(f"éªŒè¯é›†å·²ä¿å­˜åˆ°: {validation_path}")
        else:
            print(f"éªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
        
        print(f"Inductionæ•°æ®é›†å¤„ç†å®Œæˆï¼")
        
        # éªŒè¯å‡½æ•°ï¼šä»æœ¬åœ°åŠ è½½inductionæ•°æ®é›†å¹¶ç»Ÿè®¡tokenæ•°é‡
        print("\n" + "="*50)
        print("éªŒè¯inductionæ•°æ®é›†")
        print("="*50)
        
        def validate_induction_dataset():
            """éªŒè¯inductionæ•°æ®é›†ï¼Œç»Ÿè®¡tokenæ•°é‡"""
            try:
                # åŠ è½½è®­ç»ƒé›†
                train_path = "./Edge-Pruning/data/datasets/induction/train"
                if os.path.exists(train_path):
                    from datasets import load_from_disk
                    train_dataset = load_from_disk(train_path)
                    print(f"è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(train_dataset)} ä¸ªæ ·æœ¬")
                else:
                    print("è®­ç»ƒé›†æ–‡ä»¶ä¸å­˜åœ¨")
                    return
                
                # åŠ è½½éªŒè¯é›†
                validation_path = "./Edge-Pruning/data/datasets/induction/validation"
                if os.path.exists(validation_path):
                    validation_dataset = load_from_disk(validation_path)
                    print(f"éªŒè¯é›†åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(validation_dataset)} ä¸ªæ ·æœ¬")
                else:
                    print("éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨")
                    return
                
                # åŠ è½½tokenizer
                from transformers import AutoTokenizer
                tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
                print("TokenizeråŠ è½½æˆåŠŸ")
                
                def analyze_dataset_tokens(dataset, dataset_name):
                    """åˆ†ææ•°æ®é›†çš„tokenæ•°é‡"""
                    print(f"\nåˆ†æ{dataset_name}æ•°æ®é›†...")
                    
                    sentence_token_counts = []
                    corr_sentence_token_counts = []
                    
                    for i, sample in enumerate(dataset):
                        # åˆ†æsentence
                        sentence = sample['sentence']
                        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)
                        sentence_token_counts.append(len(sentence_tokens))
                        
                        # åˆ†æcorr_sentence
                        corr_sentence = sample['corr_sentence']
                        corr_sentence_tokens = tokenizer.encode(corr_sentence, add_special_tokens=False)
                        corr_sentence_token_counts.append(len(corr_sentence_tokens))
                        
                        # æ¯50ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        if (i + 1) % 50 == 0:
                            print(f"  å·²å¤„ç† {i + 1}/{len(dataset)} ä¸ªæ ·æœ¬")
                    
                    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    sentence_max = max(sentence_token_counts)
                    sentence_min = min(sentence_token_counts)
                    sentence_mean = sum(sentence_token_counts) / len(sentence_token_counts)
                    
                    corr_sentence_max = max(corr_sentence_token_counts)
                    corr_sentence_min = min(corr_sentence_token_counts)
                    corr_sentence_mean = sum(corr_sentence_token_counts) / len(corr_sentence_token_counts)
                    
                    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
                    print(f"\n{dataset_name}æ•°æ®é›†Tokenç»Ÿè®¡:")
                    print(f"  sentence:")
                    print(f"    æœ€å¤§å€¼: {sentence_max}")
                    print(f"    æœ€å°å€¼: {sentence_min}")
                    print(f"    å‡å€¼: {sentence_mean:.2f}")
                    
                    print(f"  corr_sentence:")
                    print(f"    æœ€å¤§å€¼: {corr_sentence_max}")
                    print(f"    æœ€å°å€¼: {corr_sentence_min}")
                    print(f"    å‡å€¼: {corr_sentence_mean:.2f}")
                    
                    return {
                        'sentence_max': sentence_max,
                        'sentence_min': sentence_min,
                        'sentence_mean': sentence_mean,
                        'corr_sentence_max': corr_sentence_max,
                        'corr_sentence_min': corr_sentence_min,
                        'corr_sentence_mean': corr_sentence_mean
                    }
                
                # åˆ†æè®­ç»ƒé›†
                train_stats = analyze_dataset_tokens(train_dataset, "è®­ç»ƒé›†")
                
                # åˆ†æéªŒè¯é›†
                validation_stats = analyze_dataset_tokens(validation_dataset, "éªŒè¯é›†")
                
                # ç»¼åˆç»Ÿè®¡
                print(f"\n{'='*60}")
                print("ç»¼åˆç»Ÿè®¡ç»“æœ:")
                print(f"{'='*60}")
                print(f"è®­ç»ƒé›† + éªŒè¯é›†Tokenç»Ÿè®¡:")
                print(f"  sentence:")
                print(f"    æœ€å¤§å€¼: {max(train_stats['sentence_max'], validation_stats['sentence_max'])}")
                print(f"    æœ€å°å€¼: {min(train_stats['sentence_min'], validation_stats['sentence_min'])}")
                print(f"    å‡å€¼: {(train_stats['sentence_mean'] + validation_stats['sentence_mean']) / 2:.2f}")
                
                print(f"  corr_sentence:")
                print(f"    æœ€å¤§å€¼: {max(train_stats['corr_sentence_max'], validation_stats['corr_sentence_max'])}")
                print(f"    æœ€å°å€¼: {min(train_stats['corr_sentence_min'], validation_stats['corr_sentence_min'])}")
                print(f"    å‡å€¼: {(train_stats['corr_sentence_mean'] + validation_stats['corr_sentence_mean']) / 2:.2f}")
                
                print(f"{'='*60}")
                
            except Exception as e:
                print(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
        
        # æ‰§è¡ŒéªŒè¯
        validate_induction_dataset()
        
        # è¿”å›å¤„ç†åçš„æ•°æ®é›†ä¾›è¿›ä¸€æ­¥å¤„ç†
        return final_induction_dataset
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    dataset = main()
    if dataset is not None:
        print(f"\næ•°æ®é›†åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
    else:
        print("æ•°æ®é›†åŠ è½½å¤±è´¥ï¼")
